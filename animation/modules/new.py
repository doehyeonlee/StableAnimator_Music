import os import math import argparse import numpy as np from dataclasses import dataclass import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader from einops import rearrange, repeat import imageio.v3 as iio import os import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data.distributed import DistributedSampler import os, glob, numpy as np, torch from torch.utils.data import Dataset from PIL import Image import torchvision.transforms.functional as TF def unwrap_ddp(model): return model.module if hasattr(model, "module") else model def save_model_only(path, model): os.makedirs(os.path.dirname(path), exist_ok=True) state = unwrap_ddp(model).state_dict() torch.save(state, path) print(f"[CKPT] saved model weights -> {path}") def setup_ddp(): if "RANK" in os.environ and "WORLD_SIZE" in os.environ: rank = int(os.environ["RANK"]) world_size = int(os.environ["WORLD_SIZE"]) local_rank = int(os.environ.get("LOCAL_RANK", 0)) dist.init_process_group(backend="nccl") torch.cuda.set_device(local_rank) return True, rank, local_rank, world_size else: return False, 0, 0, 1 def cleanup_ddp(): if dist.is_initialized(): dist.barrier() dist.destroy_process_group() def is_main_process(rank): return rank == 0 # --------------------------- # Utilities # --------------------------- def exists(x): return x is not None def default(val, d): return val if exists(val) else d def sinusoidal_time_embedding(timesteps, dim): """ timesteps: (B,) int64 return: (B, dim) """ device = timesteps.device half = dim // 2 freqs = torch.exp( torch.arange(half, device=device, dtype=torch.float32) * (-math.log(10000.0) / (half - 1)) ) args = timesteps.float()[:, None] * freqs[None, :] emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1) if dim % 2 == 1: emb = F.pad(emb, (0,1)) return emb # --------------------------- # FiLM Conditioning Blocks # --------------------------- class FiLM(nn.Module): """ Feature-wise Linear Modulation cond -> (gamma, beta) -> y = x * (1 + gamma) + beta """ def __init__(self, cond_dim, channels): super().__init__() self.to_gamma_beta = nn.Sequential( nn.SiLU(), nn.Linear(cond_dim, channels * 2) ) def forward(self, x, cond): """ x: (B, C, T, H, W) cond: (B, T, cond_dim) """ B, C, T, H, W = x.shape cond = self.to_gamma_beta(cond) # (B, T, 2C) cond = cond.view(B, T, 2, C).permute(0,2,3,1) # (B,2,C,T) gamma, beta = cond[:,0], cond[:,1] # (B,C,T) gamma = gamma.unsqueeze(-1).unsqueeze(-1) # (B,C,T,1,1) beta = beta.unsqueeze(-1).unsqueeze(-1) # (B,C,T,1,1) return x * (1 + gamma) + beta # --------------------------- # 3D UNet Blocks # --------------------------- class ResBlock3D(nn.Module): def __init__(self, in_ch, out_ch, cond_dim=None): super().__init__() self.in_ch = in_ch self.out_ch = out_ch self.norm1 = nn.GroupNorm(8, in_ch) self.act1 = nn.SiLU() self.conv1 = nn.Conv3d(in_ch, out_ch, 3, padding=1) self.norm2 = nn.GroupNorm(8, out_ch) self.act2 = nn.SiLU() self.conv2 = nn.Conv3d(out_ch, out_ch, 3, padding=1) self.skip = nn.Conv3d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity() # time + cond FiLM self.has_cond = cond_dim is not None self.to_film_t = nn.Sequential(nn.SiLU(), nn.Linear(256, out_ch*2)) # time emb -> (gamma,beta) self.to_film_c = nn.Sequential(nn.SiLU(), nn.Linear(cond_dim, out_ch*2)) if self.has_cond else None def forward(self, x, t_emb, c_emb=None): """ x: (B,C,T,H,W) t_emb: (B,256) c_emb: (B,T,cond_dim) or None """ h = self.norm1(x) h = self.act1(h) h = self.conv1(h) # apply FiLM from time embedding (global, broadcast to T,H,W) B, C, T, H, W = h.shape film_t = self.to_film_t(t_emb).view(B, 2, C, 1, 1, 1) # (B,2,C,1,1,1) gamma_t, beta_t = film_t[:,0], film_t[:,1] h = h * (1 + gamma_t) + beta_t # apply FiLM from condition embedding (frame-wise) if self.has_cond and exists(c_emb): film_c = self.to_film_c(c_emb) # (B,T,2C) film_c = film_c.view(B, T, 2, C).permute(0,2,3,1) # (B,2,C,T) gamma_c, beta_c = film_c[:,0], film_c[:,1] # (B,C,T) gamma_c = gamma_c.unsqueeze(-1).unsqueeze(-1) # (B,C,T,1,1) beta_c = beta_c.unsqueeze(-1).unsqueeze(-1) h = h * (1 + gamma_c) + beta_c h = self.norm2(h) h = self.act2(h) h = self.conv2(h) return h + self.skip(x) class Down3D(nn.Module): def __init__(self, ch): super().__init__() # 시간축은 유지, 공간만 downsample (2x) self.op = nn.Conv3d(ch, ch, (1,4,4), stride=(1,2,2), padding=(0,1,1)) def forward(self, x): return self.op(x) class Up3D(nn.Module): def __init__(self, ch): super().__init__() self.op = nn.ConvTranspose3d(ch, ch, (1,4,4), stride=(1,2,2), padding=(0,1,1), output_padding=(0,0,0)) def forward(self, x): return self.op(x) class SpatioTemporalUNet(nn.Module): """ 작은 3D U-Net: 시간축(T=75)은 유지, 공간(H,W)만 피라미드. - 레퍼런스 이미지(640x640) -> 작은 CNN으로 글로벌 임베딩을 뽑아 cond에 합침 - 프레임별 cond (B,T,512) + ref_emb -> FiLM 주입 """ def __init__(self, in_ch=3, base_ch=64, depth=3, cond_dim=512, t_emb_dim=256): super().__init__() self.in_ch = in_ch self.base_ch = base_ch self.depth = depth self.cond_dim = cond_dim self.t_emb_dim = t_emb_dim # 시간 임베딩 projector self.time_mlp = nn.Sequential( nn.Linear(t_emb_dim, t_emb_dim*4), nn.SiLU(), nn.Linear(t_emb_dim*4, t_emb_dim), ) # 레퍼런스 이미지 encoder (작게) self.ref_enc = nn.Sequential( nn.Conv2d(22, 32, 7, stride=2, padding=3), # 320x320 nn.SiLU(), nn.Conv2d(32, 64, 5, stride=2, padding=2), # 160x160 nn.SiLU(), nn.Conv2d(64, 128, 5, stride=2, padding=2),# 80x80 nn.SiLU(), nn.AdaptiveAvgPool2d(1), ) self.ref_proj = nn.Linear(128, cond_dim) # input conv (3D) self.in_conv = nn.Conv3d(in_ch, base_ch, 3, padding=1) # encoder chs = [base_ch] self.downs = nn.ModuleList() self.down_res = nn.ModuleList() ch = base_ch for i in range(depth): self.down_res.append(ResBlock3D(ch, ch, cond_dim=cond_dim)) self.downs.append(Down3D(ch)) chs.append(ch) mid_ch = ch # middle self.mid1 = ResBlock3D(mid_ch, mid_ch, cond_dim=cond_dim) self.mid2 = ResBlock3D(mid_ch, mid_ch, cond_dim=cond_dim) # decoder self.ups = nn.ModuleList() self.up_res = nn.ModuleList() for i in range(depth): self.ups.append(Up3D(ch)) enc_ch = chs[-(i+2)] # skip channels self.up_res.append(ResBlock3D(ch + enc_ch, enc_ch, cond_dim=cond_dim)) ch = enc_ch self.out_norm = nn.GroupNorm(8, ch) self.out_act = nn.SiLU() self.out_conv = nn.Conv3d(ch, in_ch, 3, padding=1) self.cond_pro = nn.Linear(4800, cond_dim) def forward(self, x, t, cond_seq, ref_img): """ x: (B,3,T,H,W) noisy video t: (B,) int64 timestep cond_seq: (B,T,512) ref_img: (B,3,H,W) """ B, C, T, H, W = x.shape # time emb t_emb = sinusoidal_time_embedding(t, self.t_emb_dim) t_emb = self.time_mlp(t_emb) # (B,256) # reference image -> global cond r = self.ref_enc(ref_img) # (B,128,1,1) r = r.view(B, 128) r = self.ref_proj(r) # (B,512) # merge global ref cond into frame-wise cond # (B,T,512) + (B,1,512) cond_seq = self.cond_pro(cond_seq) cond = cond_seq + r.unsqueeze(1) # UNet h = self.in_conv(x) skips = [] for res, down in zip(self.down_res, self.downs): h = res(h, t_emb, cond) skips.append(h) h = down(h) h = self.mid1(h, t_emb, cond) h = self.mid2(h, t_emb, cond) for up, res, skip in zip(self.ups, self.up_res, reversed(skips)): h = up(h) # match T,H,W (should match T by design; spatial dims may be off by 1) if h.shape[-1] != skip.shape[-1] or h.shape[-2] != skip.shape[-2]: h = F.interpolate(h, size=skip.shape[-2:], mode='trilinear', align_corners=False) h = torch.cat([h, skip], dim=1) h = res(h, t_emb, cond) h = self.out_norm(h) h = self.out_act(h) return self.out_conv(h) # predict noise ε # --------------------------- # Simple DDPM Scheduler # --------------------------- @dataclass class DDPMConfig: num_train_timesteps: int = 1000 beta_start: float = 1e-4 beta_end: float = 0.02 beta_schedule: str = "linear" # 'linear' | 'cosine' class DDPMSchedulerSimple: def __init__(self, cfg: DDPMConfig): self.cfg = cfg if getattr(cfg, 'beta_schedule', 'linear') == 'cosine': # Improved DDPM cosine schedule (Nichol & Dhariwal) T = cfg.num_train_timesteps s = 0.008 steps = torch.arange(T + 1, dtype=torch.float64) f = torch.cos(((steps / T + s) / (1 + s)) * math.pi / 2) ** 2 a_bar = (f / f[0]).clamp(min=1e-9) betas = 1 - (a_bar[1:] / a_bar[:-1]) betas = betas.clamp(1e-8, 0.999) self.betas = betas.float() else: self.betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.num_train_timesteps) self.alphas = 1.0 - self.betas self.alphas_cumprod = torch.cumprod(self.alphas, dim=0) self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1,0), value=1.0) def to(self, device): self.betas = self.betas.to(device) self.alphas = self.alphas.to(device) self.alphas_cumprod = self.alphas_cumprod.to(device) self.alphas_cumprod_prev = self.alphas_cumprod_prev.to(device) return self def add_noise(self, x0, noise, t): """ q(x_t | x_0) = sqrt(a_bar_t) * x0 + sqrt(1 - a_bar_t) * noise x0, noise: (B,3,T,H,W) t: (B,) """ a_bar = self.alphas_cumprod[t].view(-1,1,1,1,1) return torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * noise def step(self, model_pred_eps, t, x_t): """ One reverse step p(x_{t-1} | x_t) Predict x0 from eps, then compute mean of posterior. """ beta_t = self.betas[t].view(-1,1,1,1,1) a_t = self.alphas[t].view(-1,1,1,1,1) abar_t = self.alphas_cumprod[t].view(-1,1,1,1,1) abar_prev = self.alphas_cumprod_prev[t].view(-1,1,1,1,1) # estimate x0 x0 = (x_t - torch.sqrt(1 - abar_t) * model_pred_eps) / torch.sqrt(abar_t) # posterior q(x_{t-1} | x_t, x0) mean = ( torch.sqrt(abar_prev) * beta_t / (1 - abar_t) * x0 + torch.sqrt(a_t) * (1 - abar_prev) / (1 - abar_t) * x_t ) # add noise with posterior variance (beta_tilde) except for t==0 # beta_tilde_t = beta_t * (1 - abar_prev) / (1 - abar_t) noise = torch.randn_like(x_t) nonzero_mask = (t > 0).float().view(-1,1,1,1,1) var = beta_t * (1.0 - abar_prev) / (1.0 - abar_t) x_prev = mean + nonzero_mask * torch.sqrt(var) * noise return x_prev # --------------------------- # Toy Dataset (replace with real) # --------------------------- import os import glob import numpy as np from PIL import Image import torch from torch.utils.data import Dataset import torchvision.transforms.functional as TF import os, glob, numpy as np, torch from torch.utils.data import Dataset from PIL import Image import torchvision.transforms.functional as TF class VideoMusStrideDataset(Dataset): """ 각 샘플 id=A: - PNG: png_crop/A/*.png 에서 같은 랜덤 시작점 s, stride=2로 T개 선택 - MUS: mus/A.npy (shape: (N, D))에서 동일 인덱스 s, s+2, ... 로 T×D 선택 - REF: png_sam_crop/A__0100_seg.npy 를 (3,H,W)로 변환해 [-1,1] 반환: video: (3, T, H, W) in [-1,1] cond : (T, D) # D는 npy의 두 번째 차원 (여기선 4800) ref : (3, H, W) in [-1,1] """ def __init__( self, root, frames=75, size=640, stride=2, ref_source="seg", # 'seg' or 'first_frame' or 'sapiens' ids=None, # None이면 mus/*.npy 기준 자동 수집 png_dirname="png_crop", mus_dirname="mus", seg_dirname="png_sam_crop", strict_length=False, # True면 길이 부족 시 오류 seed=1234, # 랜덤 시작 인덱스 재현용 # Sapiens integration (optional) sapiens_config: str | None = None, sapiens_checkpoint: str | None = None, sapiens_device: str = "cuda:0", ): super().__init__() self.root = root self.frames = frames self.size = size self.stride = stride self.ref_source = ref_source self.png_dir = os.path.join(root, png_dirname) self.mus_dir = os.path.join(root, mus_dirname) self.seg_dir = os.path.join(root, seg_dirname) self.strict_length = strict_length self.seed = seed # Sapiens self.sapiens = None if self.ref_source == "sapiens": if sapiens_config is None or sapiens_checkpoint is None: raise ValueError("ref_source='sapiens' requires sapiens_config and sapiens_checkpoint") try: from animator_with_music.segmentation.sapiens_body_seg import SapiensBodySegmenter self.sapiens = SapiensBodySegmenter( config_path=sapiens_config, checkpoint_path=sapiens_checkpoint, device=sapiens_device, ) if not self.sapiens.ready: raise RuntimeError("SapiensBodySegmenter not ready") except Exception as e: raise RuntimeError(f"Failed to initialize SapiensBodySegmenter: {e}") # id 수집 (mus/*.npy 파일명 기준) if ids is None: paths = sorted(glob.glob(os.path.join(self.mus_dir, "*.npy"))) self.ids = [os.path.splitext(os.path.basename(p))[0] for p in paths] else: self.ids = list(ids) if not self.ids: raise RuntimeError("No ids found under mus/*.npy") def __len__(self): return len(self.ids) # ---------------- index 선택(공통) ---------------- def _choose_indices(self, num_pngs, num_mus, rng): """ PNG 개수(num_pngs)와 MUS 길이(num_mus)를 고려해 같은 랜덤 시작점에서 stride=2로 frames개 인덱스를 반환. """ need = (self.frames - 1) * self.stride + 1 max_len = min(num_pngs, num_mus) if max_len >= need: max_start = max_len - need start = int(rng.integers(0, max_start + 1)) idxs = [start + i * self.stride for i in range(self.frames)] else: # 길이 부족 if self.strict_length: raise ValueError(f"Not enough length (png={num_pngs}, mus={num_mus}) for frames={self.frames}, stride={self.stride}") # 가능한 만큼 만들고 마지막 인덱스 반복으로 패드 start = 0 idxs = [start + i * self.stride for i in range(max(1, (max_len - 1)//self.stride + 1))] idxs = idxs[:self.frames] while len(idxs) < self.frames: idxs.append(idxs[-1]) return idxs # ---------------- 로더들 ---------------- def _load_png_frames(self, A, idxs): frame_dir = os.path.join(self.png_dir, A) pngs = sorted(glob.glob(os.path.join(frame_dir, "*.png"))) if not pngs: raise FileNotFoundError(f"No PNG frames in {frame_dir}") imgs = [] last_valid = len(pngs) - 1 for i in idxs: j = min(i, last_valid) # 범위 초과시 마지막 프레임 사용 img = Image.open(pngs[j]).convert("RGB") if img.size != (self.size, self.size): img = img.resize((self.size, self.size), Image.BICUBIC) img_t = TF.to_tensor(img) # [0,1] img_t = img_t * 2.0 - 1.0 # [-1,1] imgs.append(img_t) video = torch.stack(imgs, dim=0).permute(1, 0, 2, 3) # (3,T,H,W) return video def _load_mus_cond(self, A, idxs): mus_path = os.path.join(self.mus_dir, f"{A}.npy") if not os.path.isfile(mus_path): raise FileNotFoundError(f"Missing mus file: {mus_path}") arr = np.load(mus_path) # Support (N,D) or (1,N,D) if arr.ndim == 3: _, N, D = arr.shape idxs_adj = [min(i, N - 1) for i in idxs] cond = arr[:, idxs_adj] # (1,T,D) cond_t = torch.from_numpy(cond).float().squeeze(0) # (T,D) elif arr.ndim == 2: N, D = arr.shape idxs_adj = [min(i, N - 1) for i in idxs] cond = arr[idxs_adj] # (T,D) cond_t = torch.from_numpy(cond).float() else: raise ValueError(f"Unexpected mus shape: {arr.shape}") return cond_t, D def _load_ref_seg(self, A): """ seg npy: 값 0~22 (0=background) 출력: (1, 22, self.size, self.size) # N=1 배치 """ seg_path = os.path.join(self.seg_dir, f"{A}__0100_seg.npy") if not os.path.isfile(seg_path): raise FileNotFoundError(f"Missing seg file: {seg_path}") arr = np.load(seg_path) # (H,W) or (H,W,C) # 만약 채널이 있으면 첫 채널만 사용 (라벨 맵 가정) if arr.ndim == 3: arr = arr[..., 0] # PIL로 최근접 리사이즈 (레이블 보존) # 값 범위 0~22이므로 uint8로 안전 img = Image.fromarray(arr.astype(np.uint8), mode="L") if img.size != (self.size, self.size): img = img.resize((self.size, self.size), Image.NEAREST) arr_resized = np.array(img, dtype=np.int64) # (H,W), 값 0~22 # torch 텐서로 변환 lab = torch.from_numpy(arr_resized).long() # (H,W) # one-hot: num_classes=23으로 만들고, 채널 0(배경) 버리고 1~22만 사용 oh23 = F.one_hot(lab, num_classes=23) # (H,W,23) oh_1_22 = oh23[..., 1:] # (H,W,22) # (22,H,W)로 permute 후 float32 ref = oh_1_22.permute(2, 0, 1).contiguous().float() # (22, H, W) return ref def _load_ref_seg_sapiens(self, A, idxs): """ Use Sapiens to segment a representative frame and return one-hot [22,H,W]. We take the middle frame index from idxs to align with current clip. Extra Sapiens classes beyond 22 are truncated. """ if self.sapiens is None: raise RuntimeError("Sapiens segmenter is not initialized") frame_dir = os.path.join(self.png_dir, A) pngs = sorted(glob.glob(os.path.join(frame_dir, "*.png"))) if not pngs: raise FileNotFoundError(f"No PNG frames in {frame_dir}") mid = idxs[len(idxs)//2] j = min(mid, len(pngs)-1) from PIL import Image img = Image.open(pngs[j]).convert("RGB") np_rgb = np.array(img) seg = self.sapiens.predict_mask(np_rgb) # (H,W) int # resize by nearest to target size if img.size != (self.size, self.size): imgL = Image.fromarray(seg.astype(np.int32), mode="I") imgL = imgL.resize((self.size, self.size), Image.NEAREST) seg = np.array(imgL, dtype=np.int64) lab = torch.from_numpy(seg).long() # Make one-hot with 23 classes and drop background channel 0 -> 22 channels expected by model oh = F.one_hot(lab, num_classes=max(23, int(lab.max().item())+1)) # (H,W,C) if oh.shape[-1] < 23: pad = torch.zeros((oh.shape[0], oh.shape[1], 23 - oh.shape[-1]), dtype=oh.dtype) oh = torch.cat([oh, pad], dim=-1) oh_1_22 = oh[..., 1:23] # (H,W,22) ref = oh_1_22.permute(2, 0, 1).contiguous().float() # (22,H,W) return ref def __getitem__(self, idx): A = self.ids[idx] # 재현 가능한 랜덤 시작 인덱스 (샘플 id별 고정 seed) rng = np.random.default_rng(self.seed + idx) # 길이 확인용 num_pngs = len(sorted(glob.glob(os.path.join(self.png_dir, A, "*.png")))) if num_pngs == 0: raise FileNotFoundError(f"No PNG frames in {os.path.join(self.png_dir, A)}") mus_path = os.path.join(self.mus_dir, f"{A}.npy") arr = np.load(mus_path, mmap_mode="r") num_mus = arr.shape[1] # N # 공통 인덱스 선택 idxs = self._choose_indices(num_pngs, num_mus, rng) # 로드 video = self._load_png_frames(A, idxs) # (3,T,H,W) cond, cond_dim = self._load_mus_cond(A, idxs) # (T,D) (D=4800) if self.ref_source == "seg": ref = self._load_ref_seg(A) # (22,H,W) elif self.ref_source == "sapiens": ref = self._load_ref_seg_sapiens(A, idxs) # (22,H,W) elif self.ref_source == "first_frame": ref = video[:, 0] # (3,H,W) else: raise ValueError(f"Unknown ref_source: {self.ref_source}") # 모델 cond_dim 확인/동기화는 외부에서 처리(아래 참고) return video, cond, ref # --------------------------- # Training / Inference # --------------------------- def train_one_epoch(model, sched, loader, optimizer, device, amp=True, p2_gamma=1.0, debug=True, cond_dropout=0.1): model.train() mse = nn.MSELoss(reduction="none") # <- per-sample 가중 위해 none scaler = torch.cuda.amp.GradScaler(enabled=amp) total_loss = 0.0 for batch_idx, (video, cond, ref) in enumerate(loader): video = video.to(device) # (B,3,T,H,W), in [-1,1] cond = cond.to(device) # (B,T,512) ref = ref.to(device) # (B,3,H,W) B = video.size(0) t = torch.randint(0, sched.cfg.num_train_timesteps, (B,), device=device, dtype=torch.long) noise = torch.randn_like(video) x_t = sched.add_noise(video, noise, t) optimizer.zero_grad(set_to_none=True) # Classifier-free conditioning dropout if cond_dropout > 0.0: drop_mask = (torch.rand((B,1), device=device) < cond_dropout).float() # zero-out cond/ref for a subset of batch cond_in = cond * (1.0 - drop_mask).unsqueeze(-1) ref_in = ref * (1.0 - drop_mask.view(B,1,1,1)) else: cond_in, ref_in = cond, ref with torch.cuda.amp.autocast(enabled=amp): eps_pred = model(x_t, t, cond_in, ref_in) # predict ε per_elem = mse(eps_pred, noise) # (B,3,T,H,W) per_sample = per_elem.mean(dim=(1,2,3,4)) # (B,) # ----- p2 weighting ----- abar = sched.alphas_cumprod[t] # (B,) snr = abar / (1.0 - abar) # (B,) weight = torch.pow(snr + 1.0, -p2_gamma) # (B,) # (선택) 너무 작은/큰 가중치 클리핑 # weight = weight.clamp_(min=1e-3) loss = (per_sample * weight).mean() if debug and batch_idx == 0: # 기본 통계 및 텐서 크기 확인 (첫 배치만) with torch.no_grad(): try: v_min, v_max = float(video.min().item()), float(video.max().item()) x_min, x_max = float(x_t.min().item()), float(x_t.max().item()) e_min, e_max = float(eps_pred.min().item()), float(eps_pred.max().item()) except Exception: v_min = v_max = x_min = x_max = e_min = e_max = float('nan') print("[DBG] batch=0 video", tuple(video.shape), f"range=[{v_min:.3f},{v_max:.3f}]") print("[DBG] batch=0 cond ", tuple(cond.shape)) print("[DBG] batch=0 ref ", tuple(ref.shape)) print("[DBG] batch=0 t ", tuple(t.shape), f"min={int(t.min().item())} max={int(t.max().item())}") print("[DBG] batch=0 x_t ", tuple(x_t.shape), f"range=[{x_min:.3f},{x_max:.3f}]") print("[DBG] batch=0 eps ", tuple(eps_pred.shape), f"range=[{e_min:.3f},{e_max:.3f}]") print("[DBG] batch=0 loss ", float(per_sample.mean().item())) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() total_loss += loss.item() * B return total_loss / len(loader.dataset) from tqdm import tqdm class EMA: def __init__(self, model, decay=0.999): self.decay = decay self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items() if v.dtype.is_floating_point} @torch.no_grad() def update(self, model): for k, v in model.state_dict().items(): if v.dtype.is_floating_point and k in self.shadow: self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay) @torch.no_grad() def copy_to(self, model): sd = model.state_dict() for k, v in self.shadow.items(): if k in sd: sd[k].copy_(v) model.load_state_dict(sd, strict=False) @torch.no_grad() def sample_video( model, sched, cond_seq, ref_img, device, size=640, frames=75, steps=None, show_progress: bool = True, desc: str = "Sampling", guidance_scale: float = 1.0, ): """ cond_seq: (1,T,D) ref_img : (1,C,H,W) # in [-1,1] return : (T,H,W,C) uint8 """ model.eval() steps = sched.cfg.num_train_timesteps if steps is None else steps x = torch.randn(1, 3, frames, size, size, device=device) iter_steps = range(steps - 1, -1, -1) # steps-1 ... 0 if show_progress: iter_steps = tqdm(iter_steps, total=steps, desc=desc, dynamic_ncols=True) for i in iter_steps: t = torch.full((1,), i, device=device, dtype=torch.long) if guidance_scale is None or guidance_scale <= 1.0: eps = model(x, t, cond_seq, ref_img) else: # Classifier-free guidance at inference: run with null cond/ref eps_uncond = model(x, t, torch.zeros_like(cond_seq), torch.zeros_like(ref_img)) eps_cond = model(x, t, cond_seq, ref_img) eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond) x = sched.step(eps, t, x) # 원하면 진행바에 추가 정보 표시 (예: 현재 t) if show_progress and hasattr(iter_steps, "set_postfix"): iter_steps.set_postfix(t=int(i)) x = x.clamp(-1, 1) x = (x * 127.5 + 127.5).round().byte() # [0,255] x = x[0].permute(1, 0, 2, 3).cpu().numpy() # (T,3,H,W) x = rearrange(x, "t c h w -> t h w c") return x def save_mp4(frames_uint8, path, fps=15): os.makedirs(os.path.dirname(path), exist_ok=True) iio.imwrite(path, frames_uint8, fps=fps, codec="h264") def load_model_only(path, model, map_location=None): if not os.path.isfile(path): raise FileNotFoundError(f"Checkpoint not found: {path}") state = torch.load(path, map_location=map_location or "cpu") unwrap_ddp(model).load_state_dict(state, strict=True) print(f"[CKPT] loaded model weights <- {path}") # --------------------------- # Main # --------------------------- def main(): parser = argparse.ArgumentParser() parser.add_argument("--epochs", type=int, default=1000) parser.add_argument("--batch_size", type=int, default=4) parser.add_argument("--lr", type=float, default=1e-4) parser.add_argument("--frames", type=int, default=75) parser.add_argument("--size", type=int, default=640) parser.add_argument("--base_ch", type=int, default=8) # 메모리 줄이기 위해 기본 48 parser.add_argument("--depth", type=int, default=3) parser.add_argument("--save", type=str, default="runs/sample2.mp4") parser.add_argument("--fps", type=int, default=15) parser.add_argument("--no_amp", action="store_true") parser.add_argument("--ckpt_path", type=str, default="runs/ckpt_latest.pt") parser.add_argument("--use_ema", action="store_true") parser.add_argument("--cond_dropout", type=float, default=0.1) parser.add_argument("--guidance_scale", type=float, default=1.0) parser.add_argument("--beta_schedule", type=str, default="cosine", choices=["linear","cosine"]) # cosine by default parser.add_argument("--debug", action="store_true", help="I/O와 텐서 크기/범위를 로깅") # Sapiens-related CLI (optional) # Use with ds: set ref_source='sapiens' and provide config/checkpoint parser.add_argument("--ref_source", type=str, default="seg", choices=["seg","first_frame","sapiens"], help="Reference source type") parser.add_argument("--sapiens_seg_config", type=str, default=None) parser.add_argument("--sapiens_seg_checkpoint", type=str, default=None) parser.add_argument("--sapiens_device", type=str, default="cuda:0") args = parser.parse_args() is_ddp, rank, local_rank, world_size = setup_ddp() # 앞서 안내한 함수 # DDP 초기화 이후 device = f"cuda:{local_rank}" if is_ddp else ("cuda" if torch.cuda.is_available() else "cpu") model = SpatioTemporalUNet( in_ch=3, base_ch=args.base_ch, depth=args.depth, cond_dim=4800, t_emb_dim=256, ).to(device) if is_ddp: model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True) optim = torch.optim.AdamW(model.parameters(), lr=args.lr) ema = EMA(unwrap_ddp(model)) if args.use_ema else None ds = VideoMusStrideDataset( root="/root/AIST_processed", frames=args.frames, size=args.size, stride=2, ref_source=args.ref_source, # 'seg' | 'first_frame' | 'sapiens' ids=["gBR_sBM_cAll_d04_mBR1_ch03", "gBR_sBM_cAll_d04_mBR1_ch10", "gBR_sBM_cAll_d04_mBR2_ch03", "gBR_sBM_cAll_d04_mBR2_ch07" ], # 특정 id만 학습 sapiens_config=args.sapiens_seg_config, sapiens_checkpoint=args.sapiens_seg_checkpoint, sapiens_device=args.sapiens_device, ) if is_ddp: sampler = DistributedSampler(ds, num_replicas=world_size, rank=rank, shuffle=True) shuffle = False else: sampler = None shuffle = True dl = DataLoader( ds, batch_size=args.batch_size, shuffle=shuffle, sampler=sampler, num_workers=16 if is_ddp else 8, pin_memory=True, persistent_workers=True if (is_ddp) else False, prefetch_factor=1, ) sched = DDPMSchedulerSimple(DDPMConfig(beta_schedule=args.beta_schedule)).to(device) if args.debug: print("[DBG] device:", device) tot_params = sum(p.numel() for p in unwrap_ddp(model).parameters()) train_params = sum(p.numel() for p in unwrap_ddp(model).parameters() if p.requires_grad) print(f"[DBG] model params: total={tot_params:,} trainable={train_params:,}") print(f"[DBG] dataset size: {len(ds)} ids={getattr(ds,'ids', None) if hasattr(ds,'ids') else 'n/a'}") print(f"[DBG] frames={args.frames} size={args.size} base_ch={args.base_ch} depth={args.depth}") print(f"[DBG] dataloader: batch_size={args.batch_size} workers={16 if is_ddp else 8} shuffle={shuffle}") try: batch = next(iter(dl)) video_b, cond_b, ref_b = batch print("[DBG] sample batch video:", tuple(video_b.shape), f"range=[{float(video_b.min()):.3f},{float(video_b.max()):.3f}]") print("[DBG] sample batch cond :", tuple(cond_b.shape)) print("[DBG] sample batch ref :", tuple(ref_b.shape), f"range=[{float(ref_b.min()):.3f},{float(ref_b.max()):.3f}]") except Exception as e: print("[DBG] failed to draw sample batch:", repr(e)) # train for e in tqdm(range(args.epochs), desc="Epochs", unit="epoch"): if is_ddp and sampler is not None: sampler.set_epoch(e) loss = train_one_epoch( model, sched, dl, optim, device, # e.g., f"cuda:{local_rank}" or "cuda" amp=(not args.no_amp), # 혼합정밀 옵션 p2_gamma=1.0, # p2 가중(선택) debug=(args.debug and ((not is_ddp) or rank == 0)), cond_dropout=args.cond_dropout, ) if ema is not None: ema.update(unwrap_ddp(model)) if not is_ddp or rank == 0: print(f"[Epoch {e+1}] loss={loss:.4f}") # ✅ 여기서 저장 (rank0만) if not is_ddp or rank == 0: save_model_only(args.ckpt_path, model) print(f"[CKPT] saved model weights -> {args.ckpt_path}") # Build a conditioning pair from dataset (first id) try: _, cond_s, ref_s = ds[0] except Exception as e: raise RuntimeError(f"Failed to fetch a sample for inference: {e}") cond = cond_s.unsqueeze(0).to(device) # (1,T,D) ref = ref_s.unsqueeze(0).to(device) # (1,C,H,W) # Choose model for inference (EMA if available) model_infer = unwrap_ddp(model) if ema is not None: model_ema = SpatioTemporalUNet( in_ch=3, base_ch=args.base_ch, depth=args.depth, cond_dim=4800, t_emb_dim=256, ).to(device) ema.copy_to(model_ema) model_infer = model_ema # Sample frames_uint8 = sample_video( model_infer, sched, cond, ref, device, size=args.size, frames=args.frames, steps=None, show_progress=True, desc="DDPM Inference", guidance_scale=args.guidance_scale ) # Save save_mp4(frames_uint8, args.save, fps=args.fps) print(f"[OK] saved video -> {args.save}") return if __name__ == "__main__": main() # CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train.py --epochs 100 --batch_size 4